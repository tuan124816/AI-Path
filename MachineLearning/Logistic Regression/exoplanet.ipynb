{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import copy, math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 3198)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/huytuannguyen/Desktop/FPT/My self/MachineLearning/Logistic Regression/Exoplanet Hunting in Deep Space data/exoTrain.csv')\n",
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>1</td>\n",
       "      <td>-91.91</td>\n",
       "      <td>-92.97</td>\n",
       "      <td>-78.76</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-68.00</td>\n",
       "      <td>-68.24</td>\n",
       "      <td>-75.48</td>\n",
       "      <td>-49.25</td>\n",
       "      <td>-30.92</td>\n",
       "      <td>...</td>\n",
       "      <td>139.95</td>\n",
       "      <td>147.26</td>\n",
       "      <td>156.95</td>\n",
       "      <td>155.64</td>\n",
       "      <td>156.36</td>\n",
       "      <td>151.75</td>\n",
       "      <td>-24.45</td>\n",
       "      <td>-17.00</td>\n",
       "      <td>3.23</td>\n",
       "      <td>19.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>1</td>\n",
       "      <td>989.75</td>\n",
       "      <td>891.01</td>\n",
       "      <td>908.53</td>\n",
       "      <td>851.83</td>\n",
       "      <td>755.11</td>\n",
       "      <td>615.78</td>\n",
       "      <td>595.77</td>\n",
       "      <td>458.87</td>\n",
       "      <td>492.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.50</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>-76.30</td>\n",
       "      <td>-37.84</td>\n",
       "      <td>-153.83</td>\n",
       "      <td>-136.16</td>\n",
       "      <td>38.03</td>\n",
       "      <td>100.28</td>\n",
       "      <td>-45.64</td>\n",
       "      <td>35.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>1</td>\n",
       "      <td>273.39</td>\n",
       "      <td>278.00</td>\n",
       "      <td>261.73</td>\n",
       "      <td>236.99</td>\n",
       "      <td>280.73</td>\n",
       "      <td>264.90</td>\n",
       "      <td>252.92</td>\n",
       "      <td>254.88</td>\n",
       "      <td>237.60</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.82</td>\n",
       "      <td>-53.89</td>\n",
       "      <td>-48.71</td>\n",
       "      <td>30.99</td>\n",
       "      <td>15.96</td>\n",
       "      <td>-3.47</td>\n",
       "      <td>65.73</td>\n",
       "      <td>88.42</td>\n",
       "      <td>79.07</td>\n",
       "      <td>79.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>1</td>\n",
       "      <td>3.82</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-2.88</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>3.85</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.28</td>\n",
       "      <td>...</td>\n",
       "      <td>10.86</td>\n",
       "      <td>-3.23</td>\n",
       "      <td>-5.10</td>\n",
       "      <td>-4.61</td>\n",
       "      <td>-9.82</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>-4.65</td>\n",
       "      <td>-14.55</td>\n",
       "      <td>-6.41</td>\n",
       "      <td>-2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>1</td>\n",
       "      <td>323.28</td>\n",
       "      <td>306.36</td>\n",
       "      <td>293.16</td>\n",
       "      <td>287.67</td>\n",
       "      <td>249.89</td>\n",
       "      <td>218.30</td>\n",
       "      <td>188.86</td>\n",
       "      <td>178.93</td>\n",
       "      <td>118.93</td>\n",
       "      <td>...</td>\n",
       "      <td>71.19</td>\n",
       "      <td>0.97</td>\n",
       "      <td>55.20</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-25.33</td>\n",
       "      <td>-41.31</td>\n",
       "      <td>-16.72</td>\n",
       "      <td>-14.09</td>\n",
       "      <td>27.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5087 rows × 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
       "0         2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
       "1         2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
       "2         2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
       "3         2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
       "4         2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
       "...     ...      ...      ...      ...      ...      ...      ...     ...   \n",
       "5082      1   -91.91   -92.97   -78.76   -97.33   -68.00   -68.24  -75.48   \n",
       "5083      1   989.75   891.01   908.53   851.83   755.11   615.78  595.77   \n",
       "5084      1   273.39   278.00   261.73   236.99   280.73   264.90  252.92   \n",
       "5085      1     3.82     2.09    -3.29    -2.88     1.66    -0.75    3.85   \n",
       "5086      1   323.28   306.36   293.16   287.67   249.89   218.30  188.86   \n",
       "\n",
       "       FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0      -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1      -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2      486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
       "3      311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
       "4    -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "...       ...     ...  ...        ...        ...        ...        ...   \n",
       "5082   -49.25  -30.92  ...     139.95     147.26     156.95     155.64   \n",
       "5083   458.87  492.84  ...     -26.50      -4.84     -76.30     -37.84   \n",
       "5084   254.88  237.60  ...     -26.82     -53.89     -48.71      30.99   \n",
       "5085    -0.03    3.28  ...      10.86      -3.23      -5.10      -4.61   \n",
       "5086   178.93  118.93  ...      71.19       0.97      55.20      -1.63   \n",
       "\n",
       "      FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0         48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1         -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2        -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3         20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4       -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "5082     156.36     151.75     -24.45     -17.00       3.23      19.28  \n",
       "5083    -153.83    -136.16      38.03     100.28     -45.64      35.58  \n",
       "5084      15.96      -3.47      65.73      88.42      79.07      79.43  \n",
       "5085      -9.82      -1.50      -4.65     -14.55      -6.41      -2.55  \n",
       "5086      -5.50     -25.33     -41.31     -16.72     -14.09      27.82  \n",
       "\n",
       "[5087 rows x 3198 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>FLUX.10</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.385000e-08</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>-0.001352</td>\n",
       "      <td>-9.627000e-04</td>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.001602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.888000e-08</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.000585</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-8.533000e-04</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.326400e-07</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.004564</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>4.863900e-03</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000717</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>-0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.265200e-07</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>3.113100e-03</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.107210e-06</td>\n",
       "      <td>-0.011126</td>\n",
       "      <td>-0.011189</td>\n",
       "      <td>-0.010951</td>\n",
       "      <td>-0.010575</td>\n",
       "      <td>-0.010345</td>\n",
       "      <td>-0.009983</td>\n",
       "      <td>-1.022710e-02</td>\n",
       "      <td>-0.009896</td>\n",
       "      <td>-0.009709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005944</td>\n",
       "      <td>-0.004017</td>\n",
       "      <td>-0.004017</td>\n",
       "      <td>-0.003572</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>-0.003997</td>\n",
       "      <td>-0.003846</td>\n",
       "      <td>-0.004118</td>\n",
       "      <td>-0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5082</th>\n",
       "      <td>-9.191000e-08</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>-0.000973</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>-4.925000e-04</td>\n",
       "      <td>-0.000309</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>9.897500e-07</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.005958</td>\n",
       "      <td>4.588700e-03</td>\n",
       "      <td>0.004928</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>-0.001538</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5084</th>\n",
       "      <td>2.733900e-07</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>2.548800e-03</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>3.820000e-09</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-3.000000e-07</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>3.232800e-07</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>1.789300e-03</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5087 rows × 3197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            FLUX.1    FLUX.2    FLUX.3    FLUX.4    FLUX.5    FLUX.6  \\\n",
       "0     9.385000e-08  0.000838  0.000201 -0.000270 -0.000396 -0.001247   \n",
       "1    -3.888000e-08 -0.000338 -0.000585 -0.000401 -0.000793 -0.000728   \n",
       "2     5.326400e-07  0.005359  0.005137  0.004969  0.004564  0.004660   \n",
       "3     3.265200e-07  0.003474  0.003024  0.002981  0.003177  0.003127   \n",
       "4    -1.107210e-06 -0.011126 -0.011189 -0.010951 -0.010575 -0.010345   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "5082 -9.191000e-08 -0.000930 -0.000788 -0.000973 -0.000680 -0.000682   \n",
       "5083  9.897500e-07  0.008910  0.009085  0.008518  0.007551  0.006158   \n",
       "5084  2.733900e-07  0.002780  0.002617  0.002370  0.002807  0.002649   \n",
       "5085  3.820000e-09  0.000021 -0.000033 -0.000029  0.000017 -0.000008   \n",
       "5086  3.232800e-07  0.003064  0.002932  0.002877  0.002499  0.002183   \n",
       "\n",
       "        FLUX.7        FLUX.8    FLUX.9   FLUX.10  ...  FLUX.3188  FLUX.3189  \\\n",
       "0    -0.001352 -9.627000e-04 -0.000799 -0.001602  ...  -0.000781  -0.001022   \n",
       "1    -0.000865 -8.533000e-04 -0.000840 -0.000734  ...  -0.000033  -0.000322   \n",
       "2     0.004645  4.863900e-03  0.004366  0.004844  ...  -0.000717   0.000133   \n",
       "3     0.003223  3.113100e-03  0.003124  0.003233  ...   0.000057  -0.000037   \n",
       "4    -0.009983 -1.022710e-02 -0.009896 -0.009709  ...  -0.005944  -0.004017   \n",
       "...        ...           ...       ...       ...  ...        ...        ...   \n",
       "5082 -0.000755 -4.925000e-04 -0.000309 -0.000119  ...   0.001399   0.001473   \n",
       "5083  0.005958  4.588700e-03  0.004928  0.003843  ...  -0.000265  -0.000048   \n",
       "5084  0.002529  2.548800e-03  0.002376  0.002385  ...  -0.000268  -0.000539   \n",
       "5085  0.000039 -3.000000e-07  0.000033  0.000063  ...   0.000109  -0.000032   \n",
       "5086  0.001889  1.789300e-03  0.001189  0.001307  ...   0.000712   0.000010   \n",
       "\n",
       "      FLUX.3190  FLUX.3191  FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  \\\n",
       "0     -0.001022   0.000251   0.000486   0.000925   0.000393   0.000614   \n",
       "1     -0.000322  -0.000249  -0.000049   0.000008  -0.000117   0.000065   \n",
       "2      0.000133  -0.000299  -0.000209   0.000051  -0.000118  -0.000289   \n",
       "3     -0.000037   0.000300   0.000200  -0.000127  -0.000088  -0.000173   \n",
       "4     -0.004017  -0.003572  -0.004438  -0.004385  -0.003997  -0.003846   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "5082   0.001569   0.001556   0.001564   0.001517  -0.000244  -0.000170   \n",
       "5083  -0.000763  -0.000378  -0.001538  -0.001362   0.000380   0.001003   \n",
       "5084  -0.000487   0.000310   0.000160  -0.000035   0.000657   0.000884   \n",
       "5085  -0.000051  -0.000046  -0.000098  -0.000015  -0.000046  -0.000146   \n",
       "5086   0.000552  -0.000016  -0.000055  -0.000253  -0.000413  -0.000167   \n",
       "\n",
       "      FLUX.3196  FLUX.3197  \n",
       "0      0.000051  -0.000395  \n",
       "1      0.000160   0.000199  \n",
       "2     -0.000700  -0.000967  \n",
       "3     -0.000174   0.000140  \n",
       "4     -0.004118  -0.005105  \n",
       "...         ...        ...  \n",
       "5082   0.000032   0.000193  \n",
       "5083  -0.000456   0.000356  \n",
       "5084   0.000791   0.000794  \n",
       "5085  -0.000064  -0.000025  \n",
       "5086  -0.000141   0.000278  \n",
       "\n",
       "[5087 rows x 3197 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = df\n",
    "clone = clone.drop(['LABEL'], axis=1)\n",
    "\n",
    "clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('LABEL',axis=1))\n",
    "data_scale = scaler.transform(df.drop('LABEL',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00235557, -0.00205404, -0.00579778, ...,  0.0341983 ,\n",
       "        0.02736753,  0.01805157])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df.drop(['LABEL'], axis=1)\n",
    "y_train = df['LABEL']\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = 0\n",
    "        self.bias = 0\n",
    "        self.cost_list = []\n",
    "        self.epoch_list = []\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def cost(self, X, y, w, b, lambda_ =1 ):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Args:\n",
    "        X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "        y : (array_like Shape (m,)) target value \n",
    "        w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "        b : scalar Values of bias parameter of the model\n",
    "        lambda_: unused placeholder\n",
    "        Returns:\n",
    "        total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X.shape\n",
    "        total_cost = 0.0\n",
    "        for i in range(m):\n",
    "            z_i = np.dot(X[i],w) + b\n",
    "            f_wb_i = self.sigmoid(z_i)\n",
    "            total_cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "                \n",
    "        total_cost = total_cost / m\n",
    "        return total_cost\n",
    "    \n",
    "    def gradient(self, X, y, w, b):\n",
    "        \"\"\"\n",
    "        Computes the gradient for linear regression \n",
    "    \n",
    "        Args:\n",
    "        X (ndarray (m,n): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w (ndarray (n,)): model parameters  \n",
    "        b (scalar)      : model parameter\n",
    "\n",
    "        Returns\n",
    "        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "        dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "\n",
    "        # print(X)\n",
    "        # print(y)\n",
    "\n",
    "        m,n = X.shape\n",
    "        dj_dw = np.zeros((n,))                           #(n,)\n",
    "        dj_db = 0.\n",
    "        print(dj_dw)\n",
    "\n",
    "        for i in range(m):\n",
    "            f_wb_i = self.sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "            err_i  = f_wb_i  - y[i]                       #scalar\n",
    "            dj_db = dj_db + err_i\n",
    "            \n",
    "            for j in range(n):\n",
    "                # dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "                ttt = err_i * X[i][j]\n",
    "                print(dj_dw[j] + err_i * X[i][j])\n",
    "                dj_dw[j] = dj_dw[j] + err_i * X[i][j]\n",
    "\n",
    "\n",
    "        dj_dw = dj_dw/m                                   #(n,)\n",
    "        dj_db = dj_db/m      \n",
    "        return dj_db, dj_dw\n",
    "    \n",
    "    def gradient_descent(self,X, y, beta, learning_rate):\n",
    "        y = y.reshape(-1, 1)\n",
    "        gradients = np.dot(X.T, self.sigmoid(np.dot(X, beta.T)) - y) / len(y)\n",
    "        new_betas = beta - learning_rate * gradients.T\n",
    "\n",
    "        return new_betas\n",
    "    \n",
    "    def train(self, X, y, epochs, batch_size, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Performs batch gradient descent\n",
    "        \n",
    "        Args:\n",
    "        X (ndarray (m,n)   : Data, m examples with n features\n",
    "        y (ndarray (m,))   : target values\n",
    "        alpha (float)      : Learning rate\n",
    "        epochs (scalar) : number of iterations to run gradient descent\n",
    "        \n",
    "        Returns:\n",
    "        w (ndarray (n,))   : Updated values of parameters\n",
    "        b (scalar)         : Updated value of parameter \n",
    "        \"\"\"\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros_like(X.shape[0])  #avoid modifying global w within function\n",
    "        self.bias = 0\n",
    "        total_samples = X.shape[0]\n",
    "        \n",
    "        if batch_size > total_samples: # In this case mini batch becomes same as batch gradient descent\n",
    "            batch_size = total_samples\n",
    "        \n",
    "        num_batches = int(total_samples/batch_size)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            random_indices = np.random.permutation(total_samples)\n",
    "            # print(type(random_indices))\n",
    "            # print(type(X))\n",
    "            X_tmp = X[random_indices,:]\n",
    "            y_tmp = y[random_indices]\n",
    "            \n",
    "\n",
    "            for j in range(0,total_samples,batch_size):\n",
    "                Xj = X_tmp[j:j+batch_size]\n",
    "                yj = y_tmp[j:j+batch_size]\n",
    "\n",
    "                y_pred = self.sigmoid(Xj)\n",
    "                # Calculate the gradient and update the parameters\n",
    "                dj_db, dj_dw = self.gradient(X, y, self.weights, self.bias)   \n",
    "\n",
    "                # Update Parameters using w, b, alpha and gradient\n",
    "                self.weights = self.weights - learning_rate * dj_dw               \n",
    "                self.bias = self.bias - learning_rate * dj_db            \n",
    "\n",
    "                cost = self.cost(Xj, yj, self.weights, self.bias)   \n",
    "                # cost_list.append(cost)\n",
    "\n",
    "            if i%10 == 0:\n",
    "                self.cost_list.append(cost)\n",
    "                self.epoch_list.append(i)\n",
    "        \n",
    "            \n",
    "        # return w, b, cost_list, epoch_list        #return final w,b and J history for graphing\n",
    "\n",
    "    def predict(self, X, w, b):\n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic\n",
    "        regression parameters w\n",
    "        \n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        m, n = X.shape   \n",
    "        p = np.zeros(m)\n",
    "    \n",
    "        # Loop over each example\n",
    "        for i in range(m):   \n",
    "\n",
    "            # Calculate f_wb (exactly how you did it in the compute_cost function above) \n",
    "            # using a couple of lines of code\n",
    "            f_wb = self.sigmoid(np.dot(X[i],w) + b)\n",
    "\n",
    "            # Calculate the prediction for that training example \n",
    "            if f_wb >= 0.5:\n",
    "                p[i] = 1\n",
    "            else:\n",
    "                p[i] = 0\n",
    "    \n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087, 3197) <class 'pandas.core.frame.DataFrame'>\n",
      "(5087,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, type(x_train))\n",
    "print(y_train.shape, type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLUX.1       float128\n",
       "FLUX.2       float128\n",
       "FLUX.3       float128\n",
       "FLUX.4       float128\n",
       "FLUX.5       float128\n",
       "               ...   \n",
       "FLUX.3193    float128\n",
       "FLUX.3194    float128\n",
       "FLUX.3195    float128\n",
       "FLUX.3196    float128\n",
       "FLUX.3197    float128\n",
       "Length: 3197, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = x_train.astype('float128').dtypes\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0.00353335 0.00353335 0.00353335 ... 0.00353335 0.00353335 0.00353335]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/huytuannguyen/Desktop/FPT/My self/MachineLearning/Logistic Regression/exoplanet.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m LogisticRegressionModel()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data_scale, y_train, \u001b[39m120\u001b[39;49m, \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/Users/huytuannguyen/Desktop/FPT/My self/MachineLearning/Logistic Regression/exoplanet.ipynb Cell 12\u001b[0m in \u001b[0;36mLogisticRegressionModel.train\u001b[0;34m(self, X, y, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(Xj)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# Calculate the gradient and update the parameters\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m dj_db, dj_dw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient(X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)   \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# Update Parameters using w, b, alpha and gradient\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m-\u001b[39m learning_rate \u001b[39m*\u001b[39m dj_dw               \n",
      "\u001b[1;32m/Users/huytuannguyen/Desktop/FPT/My self/MachineLearning/Logistic Regression/exoplanet.ipynb Cell 12\u001b[0m in \u001b[0;36mLogisticRegressionModel.gradient\u001b[0;34m(self, X, y, w, b)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         ttt \u001b[39m=\u001b[39m err_i \u001b[39m*\u001b[39m X[i][j]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39mprint\u001b[39m(dj_dw[j] \u001b[39m+\u001b[39m err_i \u001b[39m*\u001b[39m X[i][j])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         dj_dw[j] \u001b[39m=\u001b[39m dj_dw[j] \u001b[39m+\u001b[39m err_i \u001b[39m*\u001b[39m X[i][j]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m dj_dw \u001b[39m=\u001b[39m dj_dw\u001b[39m/\u001b[39mm                                   \u001b[39m#(n,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huytuannguyen/Desktop/FPT/My%20self/MachineLearning/Logistic%20Regression/exoplanet.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m dj_db \u001b[39m=\u001b[39m dj_db\u001b[39m/\u001b[39mm      \n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionModel()\n",
    "model.train(data_scale, y_train, 120, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huytuannguyen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_2 = LogisticRegression(random_state=0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv('/Users/huytuannguyen/Desktop/FPT/My self/MachineLearning/Logistic Regression/Exoplanet Hunting in Deep Space data/exoTrain.csv')\n",
    "dataset.shape\n",
    "x_test = testset.drop(['LABEL'], axis=1)\n",
    "y_test = testset['LABEL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 1, 2, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6587379595046197"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.score(x_test, y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365569053cc60402bfaec59c1bbbc5b0a6c079a6f3154e49d47deab19f9f4c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
